name: Advanced Continuous Proxy Checking

on:
  schedule:
    - cron: '*/10 * * * *'  # Runs every 10 minutes
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

env:
  PROXY_SOURCES: ${{ secrets.PROXY_SOURCES }}

permissions:
  contents: write
  pull-requests: write

jobs:
  update-proxy-lists:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 lxml pysocks

    - name: Debug PROXY_SOURCES
      run: |
        echo "PROXY_SOURCES length: ${#PROXY_SOURCES}"
        echo "PROXY_SOURCES first 10 characters: ${PROXY_SOURCES:0:10}..."

    - name: Process Proxy Lists
      run: |
        python -c "
        import os
        import requests
        import json
        from bs4 import BeautifulSoup
        
        def download_and_process_proxies():
            proxy_sources_str = os.environ.get('PROXY_SOURCES', '{}')
            print(f'PROXY_SOURCES: {proxy_sources_str[:50]}...')  # Print first 50 chars for debugging
            
            try:
                proxy_sources = json.loads(proxy_sources_str)
            except json.JSONDecodeError as e:
                print(f'Error decoding PROXY_SOURCES: {str(e)}')
                proxy_sources = {}
            
            if not proxy_sources:
                print('PROXY_SOURCES is empty or invalid. Using default sources.')
                proxy_sources = {
                    'http': ['https://raw.githubusercontent.com/zloi-user/hideip.me/main/http.txt'],
                    'https': ['https://raw.githubusercontent.com/zloi-user/hideip.me/main/https.txt'],
                    'socks4': ['https://raw.githubusercontent.com/zloi-user/hideip.me/main/socks4.txt'],
                    'socks5': ['https://raw.githubusercontent.com/zloi-user/hideip.me/main/socks5.txt']
                }
            
            all_proxies = {
                'http': set(),
                'https': set(),
                'socks4': set(),
                'socks5': set()
            }
            
            for proxy_type, urls in proxy_sources.items():
                for url in urls:
                    try:
                        response = requests.get(url, timeout=10)
                        proxies = set(response.text.strip().split('\n'))
                        all_proxies[proxy_type].update(proxies)
                        print(f'Successfully processed {url}')
                    except Exception as e:
                        print(f'Error processing {url}: {str(e)}')
            
            for proxy_type, proxies in all_proxies.items():
                with open(f'{proxy_type}.txt', 'w') as f:
                    for proxy in sorted(proxies):
                        f.write(f'{proxy}\n')
                print(f'Wrote {len(proxies)} {proxy_type} proxies to file')
        
        download_and_process_proxies()
        "

    - name: Check Proxies
      run: |
        python -c "
        import concurrent.futures
        import requests
        import json
        
        def check_proxy(proxy, proxy_type):
            try:
                response = requests.get('http://httpbin.org/ip', 
                                        proxies={proxy_type: proxy}, 
                                        timeout=10)
                if response.status_code == 200:
                    return proxy, True
            except:
                pass
            return proxy, False
        
        def main():
            proxy_types = ['http', 'https', 'socks4', 'socks5']
            working_proxies = {pt: [] for pt in proxy_types}
            
            for proxy_type in proxy_types:
                try:
                    with open(f'{proxy_type}.txt', 'r') as f:
                        proxies = f.read().splitlines()
                    print(f'Loaded {len(proxies)} {proxy_type} proxies')
                    
                    with concurrent.futures.ThreadPoolExecutor(max_workers=50) as executor:
                        future_to_proxy = {executor.submit(check_proxy, proxy, proxy_type): proxy for proxy in proxies}
                        for future in concurrent.futures.as_completed(future_to_proxy):
                            proxy, is_working = future.result()
                            if is_working:
                                working_proxies[proxy_type].append(proxy)
                    
                    print(f'Found {len(working_proxies[proxy_type])} working {proxy_type} proxies')
                except Exception as e:
                    print(f'Error processing {proxy_type} proxies: {str(e)}')
            
            for proxy_type in proxy_types:
                with open(f'{proxy_type}_working.txt', 'w') as f:
                    for proxy in working_proxies[proxy_type]:
                        f.write(f'{proxy}\n')
            
            with open('proxy_stats.json', 'w') as f:
                json.dump({pt: len(proxies) for pt, proxies in working_proxies.items()}, f)
            print('Wrote proxy stats to proxy_stats.json')
        
        main()
        "

    - name: Update README
      run: |
        python -c "
        import json
        from datetime import datetime
        
        def update_readme():
            try:
                with open('proxy_stats.json', 'r') as f:
                    stats = json.load(f)
            except FileNotFoundError:
                print('proxy_stats.json not found. Using empty stats.')
                stats = {'http': 0, 'https': 0, 'socks4': 0, 'socks5': 0}
            
            update_time = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')
            
            new_content = f'''# Proxy List

        Last updated: {update_time}

        ## Statistics

        - HTTP: {stats['http']} working proxies
        - HTTPS: {stats['https']} working proxies
        - SOCKS4: {stats['socks4']} working proxies
        - SOCKS5: {stats['socks5']} working proxies

        ## Usage

        You can find the working proxies in the following files:

        - HTTP: [http_working.txt](http_working.txt)
        - HTTPS: [https_working.txt](https_working.txt)
        - SOCKS4: [socks4_working.txt](socks4_working.txt)
        - SOCKS5: [socks5_working.txt](socks5_working.txt)

        These lists are updated every 10 minutes.
        '''
            
            with open('README.md', 'w') as f:
                f.write(new_content)
            print('Updated README.md')
        
        update_readme()
        "

    - name: Commit and Push Changes
      env:
        GH_PAT: ${{ secrets.GH_PAT }}
      run: |
        git config --global user.name 'github-actions[bot]'
        git config --global user.email 'github-actions[bot]@users.noreply.github.com'
        git add *.txt proxy_stats.json README.md
        git commit -m "Update proxy lists and README [skip ci]" -a || echo "No changes to commit"
        git push
