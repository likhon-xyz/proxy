name: Advanced Continuous Proxy Checking

on:
  schedule:
    - cron: '*/5 * * * *'  # Runs every 10 minutes
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

env:
  PROXY_SOURCES: ${{ secrets.PROXY_SOURCES }}

permissions:
  contents: write
  pull-requests: write

jobs:
  update-proxy-lists:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 lxml pysocks

    - name: Process Proxy Lists
      run: |
        python -c "
        import os
        import requests
        import json
        from bs4 import BeautifulSoup
        
        def download_and_process_proxies():
            proxy_sources = json.loads(os.environ['PROXY_SOURCES'])
            all_proxies = {
                'http': set(),
                'https': set(),
                'socks4': set(),
                'socks5': set()
            }
            
            for proxy_type, urls in proxy_sources.items():
                for url in urls:
                    try:
                        response = requests.get(url, timeout=10)
                        if url.endswith('.json'):
                            data = response.json()
                            # Process JSON data accordingly
                            # Example: all_proxies[proxy_type].update(data['proxies'])
                        else:
                            proxies = set(response.text.strip().split('\n'))
                            all_proxies[proxy_type].update(proxies)
                    except Exception as e:
                        print(f'Error processing {url}: {str(e)}')
            
            for proxy_type, proxies in all_proxies.items():
                with open(f'{proxy_type}.txt', 'w') as f:
                    for proxy in sorted(proxies):
                        f.write(f'{proxy}\n')
        
        download_and_process_proxies()
        "

    - name: Check Proxies
      run: |
        python -c "
        import concurrent.futures
        import requests
        import json
        
        def check_proxy(proxy, proxy_type):
            try:
                response = requests.get('http://httpbin.org/ip', 
                                        proxies={proxy_type: proxy}, 
                                        timeout=10)
                if response.status_code == 200:
                    return proxy, True
            except:
                pass
            return proxy, False
        
        def main():
            proxy_types = ['http', 'https', 'socks4', 'socks5']
            working_proxies = {pt: [] for pt in proxy_types}
            
            for proxy_type in proxy_types:
                with open(f'{proxy_type}.txt', 'r') as f:
                    proxies = f.read().splitlines()
                
                with concurrent.futures.ThreadPoolExecutor(max_workers=50) as executor:
                    future_to_proxy = {executor.submit(check_proxy, proxy, proxy_type): proxy for proxy in proxies}
                    for future in concurrent.futures.as_completed(future_to_proxy):
                        proxy, is_working = future.result()
                        if is_working:
                            working_proxies[proxy_type].append(proxy)
            
            for proxy_type in proxy_types:
                with open(f'{proxy_type}_working.txt', 'w') as f:
                    for proxy in working_proxies[proxy_type]:
                        f.write(f'{proxy}\n')
            
            with open('proxy_stats.json', 'w') as f:
                json.dump({pt: len(proxies) for pt, proxies in working_proxies.items()}, f)
        
        main()
        "

    - name: Update README
      run: |
        python -c "
        import json
        from datetime import datetime
        
        def update_readme():
            with open('proxy_stats.json', 'r') as f:
                stats = json.load(f)
            
            update_time = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')
            
            new_content = f'''# Proxy List

        Last updated: {update_time}

        ## Statistics

        - HTTP: {stats['http']} working proxies
        - HTTPS: {stats['https']} working proxies
        - SOCKS4: {stats['socks4']} working proxies
        - SOCKS5: {stats['socks5']} working proxies

        ## Usage

        You can find the working proxies in the following files:

        - HTTP: [http_working.txt](http_working.txt)
        - HTTPS: [https_working.txt](https_working.txt)
        - SOCKS4: [socks4_working.txt](socks4_working.txt)
        - SOCKS5: [socks5_working.txt](socks5_working.txt)

        These lists are updated every 10 minutes.
        '''
            
            with open('README.md', 'w') as f:
                f.write(new_content)
        
        update_readme()
        "

    - name: Commit and Push Changes
      env:
        GH_PAT: ${{ secrets.GH_PAT }}
      run: |
        git config --global user.name 'github-actions[bot]'
        git config --global user.email 'github-actions[bot]@users.noreply.github.com'
        git add *.txt proxy_stats.json README.md
        git commit -m "Update proxy lists and README [skip ci]" -a || echo "No changes to commit"
        git push
